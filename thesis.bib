@ARTICLE{Frazier2018-dq,
  title     = "A tutorial on Bayesian optimization",
  author    = "Frazier, Peter I",
  abstract  = "Bayesian optimization is an approach to optimizing objective
               functions that take a long time (minutes or hours) to evaluate.
               It is best-suited for optimization over continuous domains of
               less than 20 dimensions, and tolerates stochastic noise in
               function evaluations. It builds a surrogate for the objective
               and quantifies the uncertainty in that surrogate using a
               Bayesian machine learning technique, Gaussian process
               regression, and then uses an acquisition function defined from
               this surrogate to decide where to sample. In this tutorial, we
               describe how Bayesian optimization works, including Gaussian
               process regression and three common acquisition functions:
               expected improvement, entropy search, and knowledge gradient. We
               then discuss more advanced techniques, including running
               multiple function evaluations in parallel, multi-fidelity and
               multi-information source optimization, expensive-to-evaluate
               constraints, random environmental conditions, multi-task
               Bayesian optimization, and the inclusion of derivative
               information. We conclude with a discussion of Bayesian
               optimization software and future research directions in the
               field. Within our tutorial material we provide a generalization
               of expected improvement to noisy evaluations, beyond the
               noise-free setting where it is more commonly applied. This
               generalization is justified by a formal decision-theoretic
               argument, standing in contrast to previous ad hoc modifications.",
  publisher = "arXiv",
  year      =  2018
}

@ARTICLE{Sobol1967-nw,
  title     = "On the distribution of points in a cube and the approximate
               evaluation of integrals",
  author    = "Sobol, I M",
  journal   = "U.S.S.R. Comput. Math. Math. Phys.",
  publisher = "Elsevier BV",
  volume    =  7,
  number    =  4,
  pages     = "86--112",
  month     =  jan,
  year      =  1967,
  language  = "en"
}

@BOOK{Rasmussen2005-ou,
  title     = "Gaussian processes for machine learning",
  author    = "Rasmussen, Carl Edward and Williams, Christopher K I",
  publisher = "MIT Press",
  series    = "Adaptive Computation and Machine Learning series",
  month     =  nov,
  year      =  2005,
  address   = "London, England",
  language  = "en"
}

@article{Duvenaud2014,
  doi = {10.17863/CAM.14087},
  url = {https://www.repository.cam.ac.uk/handle/1810/247281},
  author = {Duvenaud,  David},
  keywords = {Machine learning,  Statistics,  FOS: Mathematics,  FOS: Mathematics,  Forecasting,  Model building,  Gaussian processes,  Time series},
  language = {en},
  title = {Automatic model construction with Gaussian processes},
  publisher = {Apollo - University of Cambridge Repository},
  year = {2014},
  copyright = {Attribution-ShareAlike 2.0 UK: England & Wales}
}

@article{Monte-Catlo-botorch,
  doi = {10.48550/ARXIV.1910.06403},
  url = {https://arxiv.org/abs/1910.06403},
  author = {Balandat,  Maximilian and Karrer,  Brian and Jiang,  Daniel R. and Daulton,  Samuel and Letham,  Benjamin and Wilson,  Andrew Gordon and Bakshy,  Eytan},
  keywords = {Machine Learning (cs.LG),  Distributed,  Parallel,  and Cluster Computing (cs.DC),  Optimization and Control (math.OC),  Machine Learning (stat.ML),  FOS: Computer and information sciences,  FOS: Computer and information sciences,  FOS: Mathematics,  FOS: Mathematics},
  title = {BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}